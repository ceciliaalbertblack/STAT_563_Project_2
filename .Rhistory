name = "Complex",
values = c("#1f78b4", "#33a02c", "#e31a1c", "#ff7f00", "#6a3d9a"),
labels = c("Free Cu", "Cu(CO3)2", "Cu(OH)2", "CuCl", "CuHOC3")
) +
geom_point(size = 1, alpha = 0.8) +
geom_smooth(aes(color = Complex), method = "lm", se = TRUE, linewidth = 1) +
facet_wrap(~Typology) +
labs(
x = "pH",
y = "log Concentration (nM)",
color = "Cu Complex"
) +
theme(
legend.position = "top",
panel.grid.minor = element_blank()
)+
theme_minimal()
setwd("C:/Users/q4q/repos/STAT_563_Project_2")
# Load necessary library
library(ggplot2)
# Define the logistic density function
f_logistic <- function(x, mu, s) {
exp(-(x - mu)/s) / (s * (1 + exp(-(x - mu)/s))^2)
}
# Create a sequence of x values for plotting
x <- seq(-10, 10, length.out = 1000)
# --- (a) Effect of scale parameter s ---
mu <- 0
s_values <- c(0.5, 1, 2)
# Compute densities for each s
dens_scale <- data.frame(
x = rep(x, times = length(s_values)),
f = unlist(lapply(s_values, function(s) f_logistic(x, mu, s))),
s = factor(rep(s_values, each = length(x)))
)
# Plot: Effect of scale
ggplot(dens_scale, aes(x = x, y = f, color = s)) +
geom_line(size = 1) +
labs(
title = "Effect of Scale Parameter (s) on Logistic Density",
x = "x",
y = "Density",
color = "Scale (s)"
) +
theme_minimal(base_size = 14)
s <- 1
mu_values <- c(-2, 0, 2)
# Compute densities for each mu
dens_location <- data.frame(
x = rep(x, times = length(mu_values)),
f = unlist(lapply(mu_values, function(mu) f_logistic(x, mu, s))),
mu = factor(rep(mu_values, each = length(x)))
)
# Plot: Effect of location
ggplot(dens_location, aes(x = x, y = f, color = mu)) +
geom_line(size = 1) +
labs(
title = "Effect of Location Parameter (μ) on Logistic Density",
x = "x",
y = "Density",
color = "Location (μ)"
) +
theme_minimal(base_size = 14)
# Load required library
library(ggplot2)
# Define the logistic PDF
f_logistic <- function(x, mu = 0, s = 1) {
exp(-(x - mu)/s) / (s * (1 + exp(-(x - mu)/s))^2)
}
# Create a sequence of x values
x <- seq(-6, 6, length.out = 1000)
# Compute densities
densities <- data.frame(
x = x,
Logistic = f_logistic(x, mu = 0, s = 1),
Normal = dnorm(x, mean = 0, sd = 1)
)
# Convert to long format for plotting
dens_long <- tidyr::pivot_longer(densities, cols = c("Logistic", "Normal"),
names_to = "Distribution", values_to = "Density")
# Plot overlay
ggplot(dens_long, aes(x = x, y = Density, color = Distribution)) +
geom_line(size = 1) +
labs(
title = "Comparison of Standard Logistic and Standard Normal Densities",
x = "x",
y = "Density",
color = "Distribution"
) +
theme_minimal(base_size = 14)
# Create a random dataset
set.seed(123)  # for reproducibility
# Generate a random dataset of 50 points from a normal distribution
n <- 50
X <- rnorm(n, mean = 5, sd = 2)
# Optional: add a few outliers
X <- c(X, 20, 25)
# Quick look at the dataset
X
# Create a random dataset
set.seed(123)  # for reproducibility
# Generate a random dataset of 50 points from a normal distribution
n <- 50
X <- rnorm(n, mean = 5, sd = 2)
# Optional: add a few outliers
X <- c(X, 20, 25)
# Quick look at the dataset
hist(X)
#
n <- length(X)
# Define a function to compute mu_hat and s_hat iteratively
estimate_mu_s <- function(X, tol = 1e-6, max_iter = 1000) {
# Initialize
mu_hat <- median(X)   # robust starting value
s_hat <- mean(abs(X - mu_hat))  # initial scale estimate
for (iter in 1:max_iter) {
# Compute weights
z <- (X - mu_hat) / (2 * s_hat)
w <- ifelse(z == 0, 1, tanh(z) / z)
# Update mu_hat
mu_new <- sum(X * w) / sum(w)
# Update s_hat
s_new <- mean(abs(X - mu_new) * w)
# Check convergence
if (abs(mu_new - mu_hat) < tol && abs(s_new - s_hat) < tol) {
break
}
mu_hat <- mu_new
s_hat <- s_new
}
list(mu_hat = mu_hat, s_hat = s_hat, weights = w, iterations = iter)
}
# Run the function
result <- estimate_mu_s(X)
# Show results
result$mu_hat
result$s_hat
result$weights
#
n <- length(X)
# Define a function to compute mu_hat and s_hat iteratively
estimate_mu_s <- function(X, tol = 1e-6, max_iter = 1000) {
# Initialize
mu_hat <- median(X)   # robust starting value
s_hat <- mean(abs(X - mu_hat))  # initial scale estimate
for (iter in 1:max_iter) {
# Compute weights
z <- (X - mu_hat) / (2 * s_hat)
w <- ifelse(z == 0, 1, tanh(z) / z)
# Update mu_hat
mu_new <- sum(X * w) / sum(w)
# Update s_hat
s_new <- mean(abs(X - mu_new) * w)
# Check convergence
if (abs(mu_new - mu_hat) < tol && abs(s_new - s_hat) < tol) {
break
}
mu_hat <- mu_new
s_hat <- s_new
}
list(mu_hat = mu_hat, s_hat = s_hat, weights = w, iterations = iter)
}
# Run the function
result <- estimate_mu_s(X)
# Show results
result$mu_hat
result$s_hat
result$weights
#
n <- length(X)
# Define a function to compute mu_hat and s_hat iteratively
estimate_mu_s <- function(X, tol = 1e-6, max_iter = 1000) {
# Initialize
mu_hat <- median(X)   # robust starting value
s_hat <- mean(abs(X - mu_hat))  # initial scale estimate
for (iter in 1:max_iter) {
# Compute weights
z <- (X - mu_hat) / (2 * s_hat)
w <- ifelse(z == 0, 1, tanh(z) / z)
# Update mu_hat
mu_new <- sum(X * w) / sum(w)
# Update s_hat
s_new <- mean(abs(X - mu_new) * w)
# Check convergence
if (abs(mu_new - mu_hat) < tol && abs(s_new - s_hat) < tol) {
break
}
mu_hat <- mu_new
s_hat <- s_new
}
list(mu_hat = mu_hat, s_hat = s_hat, weights = w, iterations = iter)
}
# Run the function
result <- estimate_mu_s(X)
# View results
cat("Results of Robust Estimation:\n")
cat("-----------------------------\n")
cat("Estimated mu_hat:", result$mu_hat, "\n")
cat("Estimated s_hat:", result$s_hat, "\n")
cat("Number of iterations:", result$iterations, "\n")
cat("\nWeights for each observation:\n")
print(result$weights)
# Load necessary library
library(stats)
# Step 1: Generate random numbers from a logistic distribution
set.seed(123)  # For reproducibility
n <- 200
true_mu <- 0
true_s <- 1
# Logistic random variables
x <- rlogis(n, location = true_mu, scale = true_s)
# Step 2: Define the log-likelihood and its derivatives
loglik <- function(params, data) {
mu <- params[1]
s  <- params[2]
n  <- length(data)
# Logistic log-likelihood
ll <- -n*log(s) - 2*sum(log(1 + exp(-(data - mu)/s)))
return(ll)
}
# Score function (gradient of log-likelihood)
score <- function(params, data) {
mu <- params[1]
s  <- params[2]
z <- (data - mu)/s
w <- exp(-z) / (1 + exp(-z))
# Partial derivatives
dL_dmu <- sum((1 - 2*w)/s)
dL_ds  <- -length(data)/s + sum(z * (1 - 2*w)/s)
return(c(dL_dmu, dL_ds))
}
# Fisher information matrix (expected value of negative second derivative)
fisher_info <- function(params, data) {
mu <- params[1]
s  <- params[2]
# Expected Fisher information for logistic distribution
n <- length(data)
I11 <- n * (pi^2) / (3 * s^2)       # w.r.t mu^2
I12 <- 0                            # cross term
I22 <- n * (pi^2) / (3 * s^2)       # w.r.t s^2
I <- matrix(c(I11, I12, I12, I22), nrow = 2)
return(I)
}
# Step 3: Fisher Scoring Algorithm
fisher_scoring <- function(data, tol = 1e-6, max_iter = 100) {
# Initial guesses
mu_hat <- mean(data)
s_hat  <- sd(data) * sqrt(3)/pi  # Approximation for logistic scale
params <- c(mu_hat, s_hat)
for (i in 1:max_iter) {
score_val <- score(params, data)
info_val  <- fisher_info(params, data)
# Update parameters
params_new <- params + solve(info_val) %*% score_val
# Check convergence
if (max(abs(params_new - params)) < tol) {
cat("Converged in", i, "iterations\n")
break
}
params <- params_new
}
names(params) <- c("mu_hat", "s_hat")
return(params)
}
# Step 4: Run Fisher scoring to estimate MLEs
mle_estimates <- fisher_scoring(x)
print(mle_estimates)
# Extract MLEs from part b
mu_hat <- mle_estimates["mu_hat"]
s_hat  <- mle_estimates["s_hat"]
n      <- length(x)
# Construct the Fisher information matrix for the logistic distribution, evaluated at the MLEs
I11 <- n * (pi^2)/(3 * s_hat^2)
I22 <- n * (pi^2)/(3 * s_hat^2)
I12 <- 0
F_obs <- matrix(c(I11, I12, I12, I22), nrow = 2)
# Variance-covariance matrix
vcov <- solve(F_obs)
# Standard errors
se_mu <- sqrt(vcov[1,1])
se_s  <- sqrt(vcov[2,2])
# 95% confidence intervals
z <- qnorm(0.975)
ci_mu <- mu_hat + c(-1,1)*z*se_mu
ci_s  <- s_hat  + c(-1,1)*z*se_s
ci_mu
ci_s
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s"),
Method = c("Fisher Information", "Fisher Information"),
Lower = c(ci_mu[1], ci_s[1]),
Upper = c(ci_mu[2], ci_s[2])
)
# Add the non-parametric CIs as separate rows
ci_table <- rbind(
ci_table,
data.frame(
Parameter = c("mu", "s"),
Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower = c(ci_mu_np[1], ci_s_np[1]),
Upper = c(ci_mu_np[2], ci_s_np[2])
)
)
# Non-parametric CI for mu (location) using the empirical CDF
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))
ci_mu_np
# Non-parametric CI for s (scale) using bootstrap
set.seed(123)   # for reproducibility
B <- 1000       # number of bootstrap samples
s_boot <- replicate(B, {
xb <- sample(x, replace = TRUE)       # bootstrap sample
diff(range(xb)) / (pi/sqrt(3))        # approximate logistic scale
})
# 95% CI for s from bootstrap percentiles
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))
ci_s_np
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s"),
Method = c("Fisher Information", "Fisher Information"),
Lower = c(ci_mu[1], ci_s[1]),
Upper = c(ci_mu[2], ci_s[2])
)
# Add the non-parametric CIs as separate rows
ci_table <- rbind(
ci_table,
data.frame(
Parameter = c("mu", "s"),
Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower = c(ci_mu_np[1], ci_s_np[1]),
Upper = c(ci_mu_np[2], ci_s_np[2])
)
)
# Print the tabl
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s"),
Method = c("Fisher Information", "Fisher Information"),
Lower = c(ci_mu[1], ci_s[1]),
Upper = c(ci_mu[2], ci_s[2])
)
# Add the non-parametric CIs as separate rows
ci_table <- rbind(
ci_table,
data.frame(
Parameter = c("mu", "s"),
Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower = c(ci_mu_np[1], ci_s_np[1]),
Upper = c(ci_mu_np[2], ci_s_np[2])
)
)
# Print the table
print(ci_table)
ci_table
# 95% CI for mu using empirical percentiles
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))
# 95% CI for s using bootstrap of standard deviation
set.seed(123)
B <- 1000
s_boot <- replicate(B, {
xb <- sample(x, replace = TRUE)
sd(xb) * sqrt(3)/pi   # approximate logistic scale
})
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s"),
Method = c("Fisher Information", "Fisher Information"),
Lower = c(ci_mu[1], ci_s[1]),
Upper = c(ci_mu[2], ci_s[2])
)
# Add the non-parametric CIs as separate rows
ci_table <- rbind(
ci_table,
data.frame(
Parameter = c("mu", "s"),
Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower = c(ci_mu_np[1], ci_s_np[1]),
Upper = c(ci_mu_np[2], ci_s_np[2])
)
)
# Print the table
print(ci_table)
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s", "mu", "s"),
Method    = c("Fisher Information", "Fisher Information",
"Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower     = c(ci_mu_fisher[1], ci_s_fisher[1], ci_mu_np[1], ci_s_np[1]),
Upper     = c(ci_mu_fisher[2], ci_s_fisher[2], ci_mu_np[2], ci_s_np[2])
)
# Extract MLEs from part b
mu_hat <- mle_estimates["mu_hat"]
s_hat  <- mle_estimates["s_hat"]
n      <- length(x)
# Construct the Fisher information matrix for the logistic distribution, evaluated at the MLEs
I11 <- n * (pi^2)/(3 * s_hat^2)
I22 <- n * (pi^2)/(3 * s_hat^2)
I12 <- 0
F_obs <- matrix(c(I11, I12, I12, I22), nrow = 2)
# Variance-covariance matrix
vcov <- solve(F_obs)
# Standard errors
se_mu <- sqrt(vcov[1,1])
se_s  <- sqrt(vcov[2,2])
# 95% confidence intervals
z <- qnorm(0.975)
ci_mu_fisher <- mu_hat + c(-1,1)*z*se_mu
ci_s_fisher  <- s_hat  + c(-1,1)*z*se_s
ci_mu
ci_s
# 95% CI for mu using empirical percentiles
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))
# 95% CI for s using bootstrap of standard deviation
set.seed(123)
B <- 1000
s_boot <- replicate(B, {
xb <- sample(x, replace = TRUE)
sd(xb) * sqrt(3)/pi   # approximate logistic scale
})
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s", "mu", "s"),
Method    = c("Fisher Information", "Fisher Information",
"Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower     = c(ci_mu_fisher[1], ci_s_fisher[1], ci_mu_np[1], ci_s_np[1]),
Upper     = c(ci_mu_fisher[2], ci_s_fisher[2], ci_mu_np[2], ci_s_np[2])
)
# Print the table
print(ci_table)
# Extract MLEs from part b
mu_hat <- mle_estimates["mu_hat"]
s_hat  <- mle_estimates["s_hat"]
n      <- length(x)
# Construct the Fisher information matrix for the logistic distribution, evaluated at the MLEs
I11 <- n * (pi^2)/(3 * s_hat^2)
I22 <- n * (pi^2)/(3 * s_hat^2)
I12 <- 0
F_obs <- matrix(c(I11, I12, I12, I22), nrow = 2)
# Variance-covariance matrix
vcov <- solve(F_obs)
# Standard errors
se_mu <- sqrt(vcov[1,1])
se_s  <- sqrt(vcov[2,2])
# 95% confidence intervals
z <- qnorm(0.975)
ci_mu <- mu_hat + c(-1,1)*z*se_mu
ci_s  <- s_hat  + c(-1,1)*z*se_s
ci_mu
ci_s
# Non-parametric CI for mu (location) using the empirical CDF
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))
ci_mu_np
# Non-parametric CI for s (scale) using bootstrap
set.seed(123)   # for reproducibility
B <- 1000       # number of bootstrap samples
s_boot <- replicate(B, {
xb <- sample(x, replace = TRUE)       # bootstrap sample
diff(range(xb)) / (pi/sqrt(3))        # approximate logistic scale
})
# 95% CI for s from bootstrap percentiles
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))
ci_s_np
# Create a data frame to display the results
ci_table <- data.frame(
Parameter = c("mu", "s"),
Method = c("Fisher Information", "Fisher Information"),
Lower = c(ci_mu[1], ci_s[1]),
Upper = c(ci_mu[2], ci_s[2])
)
# Add the non-parametric CIs as separate rows
ci_table <- rbind(
ci_table,
data.frame(
Parameter = c("mu", "s"),
Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
Lower = c(ci_mu_np[1], ci_s_np[1]),
Upper = c(ci_mu_np[2], ci_s_np[2])
)
)
# Print the table
print(ci_table)
# Load necessary library
library(stats)
# Step 1: Generate random numbers from a logistic distribution
set.seed(123)  # For reproducibility
n <- 200
true_mu <- 0
true_s <- 1
# Logistic random variables
x <- rlogis(n, location = true_mu, scale = true_s)
# Estimate mean and scale
mu_hat <- mean(x)
s_hat  <- sd(x) * sqrt(3)/pi   # approximate logistic scale from SD
# Variance of the sample mean using logistic formula
var_mu <- (pi^2 * s_hat^2) / (3 * n)  # Var(mean) = Var(X)/n
se_mu  <- sqrt(var_mu)
# 95% Wald CI
z <- qnorm(0.975)
ci_wald <- mu_hat + c(-1,1) * z * se_mu
ci_wald
set.seed(123)
B <- 1000  # number of bootstrap samples
mu_boot <- replicate(B, {
xb <- sample(x, replace = TRUE)
mean(xb)
})
# 95% percentile bootstrap CI
ci_boot <- quantile(mu_boot, probs = c(0.025, 0.975))
ci_boot
