---
title: "STAT_563_project_2_Q2"
author: "Cecilia Albert-Black"
date: "2025-11-03"
output: html_document
---

This project integrated large-sample theory, likelihood-based inference, and re-sampling techniques in modern computational statistics. In this Markdown file, I will:

-   Examine the logistic distribution and its importance in statistical modeling and machine learning

-   compute confidence intervals based on Fisher information

-   Explore bootstrapping and the distribution of correlation matrices

-   Implement slice sampling for correlation uncertainty

### Finding the Maximum Likelihood Estimates (MLEs)

Given observations $x_1, \ldots, x_n$, the likelihood function is given by

$$
L(\mu, s) = s^{-n} \prod_{i=1}^{n} 
\frac{\exp\left(-\frac{x_i - \mu}{s}\right)}
{\left(1 + \exp\left(-\frac{x_i - \mu}{s}\right)\right)^2}.
$$

and taking logarithms we have the log-likelihood

$$
\ell(\mu, s) = -n \log s 
- \frac{1}{s} \sum_{i=1}^{n} (x_i - \mu)
- 2 \sum_{i=1}^{n} \log\left(1 + \exp\left(-\frac{x_i - \mu}{s}\right)\right).
$$

Let $z_i = (x_i - \mu)/s$, we obtain the score equations:

$$
\frac{\partial \ell}{\partial \mu} 
= \frac{1}{s} \sum_{i=1}^{n} \tanh\left(\frac{z_i}{2}\right),
$$

$$
\frac{\partial \ell}{\partial s} 
= -\frac{n}{s} 
+ \frac{1}{s^2} \sum_{i=1}^{n} (x_i - \mu)
- \frac{2}{s^2} \sum_{i=1}^{n} (x_i - \mu) 
\frac{e^{-z_i}}{1 + e^{-z_i}}.
$$

So, the score vector is:

$$
S = 
\begin{pmatrix}
\frac{\partial \ell}{\partial \mu} \\
\frac{\partial \ell}{\partial s}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{s} \sum_{i=1}^{n} \tanh\left(\frac{z_i}{2}\right) \\
-\frac{n}{s} + \frac{1}{s^2} \sum_{i=1}^{n} (x_i - \mu)
- \frac{2}{s^2} \sum_{i=1}^{n} (x_i - \mu)
\frac{e^{-z_i}}{1 + e^{-z_i}}
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0
\end{pmatrix}.
$$

#### a. Calculating the MLEs and weights

$$ \hat{\mu}, \hat{s} $$ are equal to

$$
\sum_{i=1}^{n} \tanh\left( \frac{X_i - \hat{\mu}}{2 \hat{s}} \right) = 0, \hat{s} = \frac{1}{n} \sum_{i=1}^{n} \lvert X_i - \hat{\mu} \rvert \, w_i
$$

```{r}
# Create a random dataset
set.seed(123)  # for reproducibility

# Generate a random dataset of 50 points from a normal distribution
n <- 50
X <- rnorm(n, mean = 5, sd = 2)  

# Optional: add a few outliers
X <- c(X, 20, 25)

# Quick look at the dataset
hist(X)
```

```{r}
# 
n <- length(X)

# Define a function to compute mu_hat and s_hat iteratively
estimate_mu_s <- function(X, tol = 1e-6, max_iter = 1000) {
  
  # Initialize
  mu_hat <- median(X)   # robust starting value
  s_hat <- mean(abs(X - mu_hat))  # initial scale estimate
  
  for (iter in 1:max_iter) {
    
    # Compute weights
    z <- (X - mu_hat) / (2 * s_hat)
    w <- ifelse(z == 0, 1, tanh(z) / z)
    
    # Update mu_hat
    mu_new <- sum(X * w) / sum(w)
    
    # Update s_hat
    s_new <- mean(abs(X - mu_new) * w)
    
    # Check convergence
    if (abs(mu_new - mu_hat) < tol && abs(s_new - s_hat) < tol) {
      break
    }
    
    mu_hat <- mu_new
    s_hat <- s_new
  }
  
  list(mu_hat = mu_hat, s_hat = s_hat, weights = w, iterations = iter)
}

# Run the estimation
result <- estimate_mu_s(X)

# Create a summary data frame
summary_table <- data.frame(
  Parameter = c("Estimated μ̂", "Estimated ŝ", "Iterations"),
  Value = c(result$mu_hat, result$s_hat, result$iterations)
)

# Print results
print(summary_table, row.names = FALSE)

# Show first few weights in tabular form
weights_table <- data.frame(
  Observation = 1:length(result$weights),
  Weight = result$weights
)

print(head(weights_table, 10), row.names = FALSE)


```

#### b. Fisher Information

Taking the second partial derivative, the observed information at ($$ \hat{\mu}, \hat{s} $$) is:

$$
\mathcal{F}_{\text{obs}}(\hat{\mu}, \hat{s}) = - \nabla^2 \ell(\hat{\mu}, \hat{s}) = - \mathcal{H}(\hat{\mu}, \hat{s}).
$$

The expected Fisher information per observation $$
\mathcal{F}(\mu, s) = 
\begin{pmatrix}
\dfrac{1}{3 s^2} & 0 \\[2mm]
0 & \dfrac{1}{s^2} \left( \dfrac{\pi^2}{3} - 1 \right)
\end{pmatrix}, 
\quad 
\mathcal{F}_n(\mu, s) = n \, \mathcal{F}(\mu, s).
$$

Generate random numbers from logistic distribution of sample size n = 200 observations and write a Fisher scoring iterative algorithm to find the MLEs and estimate the parameters (μ, s) of the logistic distribution.

```{r}
# Load necessary library
library(stats)

# Step 1: Generate random numbers from a logistic distribution
set.seed(123)  # For reproducibility
n <- 200
true_mu <- 0
true_s <- 1

# Logistic random variables
x <- rlogis(n, location = true_mu, scale = true_s)

# Step 2: Define the log-likelihood and its derivatives
loglik <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  n  <- length(data)
  
  # Logistic log-likelihood
  ll <- -n*log(s) - 2*sum(log(1 + exp(-(data - mu)/s)))
  return(ll)
}

# Score function (gradient of log-likelihood)
score <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  
  z <- (data - mu)/s
  w <- exp(-z) / (1 + exp(-z))
  
  # Partial derivatives
  dL_dmu <- sum((1 - 2*w)/s)
  dL_ds  <- -length(data)/s + sum(z * (1 - 2*w)/s)
  
  return(c(dL_dmu, dL_ds))
}

# Fisher information matrix (expected value of negative second derivative)
fisher_info <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  
  # Expected Fisher information for logistic distribution
  n <- length(data)
  I11 <- n * (pi^2) / (3 * s^2)       # w.r.t mu^2
  I12 <- 0                            # cross term
  I22 <- n * (pi^2) / (3 * s^2)       # w.r.t s^2
  
  I <- matrix(c(I11, I12, I12, I22), nrow = 2)
  return(I)
}

# Step 3: Fisher Scoring Algorithm
fisher_scoring <- function(data, tol = 1e-6, max_iter = 100) {
  # Initial guesses
  mu_hat <- mean(data)
  s_hat  <- sd(data) * sqrt(3)/pi  # Approximation for logistic scale
  
  params <- c(mu_hat, s_hat)
  
  for (i in 1:max_iter) {
    score_val <- score(params, data)
    info_val  <- fisher_info(params, data)
    
    # Update parameters
    params_new <- params + solve(info_val) %*% score_val
    
    # Check convergence
    if (max(abs(params_new - params)) < tol) {
      cat("Converged in", i, "iterations\n")
      break
    }
    
    params <- params_new
  }
  
  names(params) <- c("mu_hat", "s_hat")
  return(params)
}

# Step 4: Run Fisher scoring to estimate MLEs
mle_estimates <- fisher_scoring(x)

mle_table <- data.frame(
  Parameter = names(mle_estimates),
  Estimate  = round(as.numeric(mle_estimates), 6)
)

# Print as a table
print(mle_table, row.names = FALSE)

```

Output explanation according to ChatGPT:

1.  Convergence message
    -   This means the iterative Fisher scoring algorithm stopped after 77 iterations because the parameter estimates changed less than the specified tolerance (`tol = 1e-6`) between successive updates.
    -   Essentially, the algorithm believes it has reached the MLEs.
2.  Parameter estimates
    -   `mu_hat = 0.032` is the MLE for the location parameter μ of your logistic distribution. Since the true μ used to generate the data was 0, this estimate is very close—slight deviation is normal due to random sampling.
    -   `s_hat = 0.905` is the MLE for the scale parameter s of your logistic distribution. The true scale used was 1, so again this estimate is slightly lower, which is reasonable for a sample of size 200.

#### c. Large-sample Wald intervals

By inverting the Fisher information ( $$ \mathcal{F}_{\text{obs}} $$ ) or using the empirical CDF ( $$ \mathcal{F}_{\text{n}} $$ ) and using the generated data in part b, compute the confidence intervals:

1.  Using the Fisher information: more common in MLE theory

```{r}
# Extract MLEs from part b
mu_hat <- mle_estimates["mu_hat"]
s_hat  <- mle_estimates["s_hat"]
n      <- length(x)

# Construct the Fisher information matrix for the logistic distribution, evaluated at the MLEs
I11 <- n * (pi^2)/(3 * s_hat^2)
I22 <- n * (pi^2)/(3 * s_hat^2)
I12 <- 0
F_obs <- matrix(c(I11, I12, I12, I22), nrow = 2)

# Variance-covariance matrix
vcov <- solve(F_obs)

# Standard errors
se_mu <- sqrt(vcov[1,1])
se_s  <- sqrt(vcov[2,2])

# 95% confidence intervals
z <- qnorm(0.975)
ci_mu <- mu_hat + c(-1,1)*z*se_mu
ci_s  <- s_hat  + c(-1,1)*z*se_s

# Results table
results_table <- data.frame(
  Estimate  = c(mu_hat, s_hat),
  Std_Error = c(se_mu, se_s),
  CI_Lower  = c(ci_mu[1], ci_s[1]),
  CI_Upper  = c(ci_mu[2], ci_s[2])
)
results_table
```

2.  Using the empirical CDF (non-parametric): less common but useful for CIs

```{r}
# Estimating mu (location) 
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))

# Estimating s (scale)
set.seed(123)   # for reproducibility
B <- 1000       # number of bootstrap samples

s_boot <- replicate(B, {
  xb <- sample(x, replace = TRUE)       # bootstrap sample
  diff(range(xb)) / (pi/sqrt(3))        # approximate logistic scale
})

# 95% CI for s from bootstrap percentiles
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))

# Create a table for the CIs
ci_table <- data.frame(
  Parameter = c("mu_hat", "s_hat"),
  CI_Lower = c(ci_mu_np[1], ci_s_np[1]),
  CI_Upper = c(ci_mu_np[2], ci_s_np[2])
)

# Print the table
print(ci_table, row.names = FALSE)

```

Summary table

```{r}
# Create a data frame to display the results
ci_table <- data.frame(
  Parameter = c("mu", "s"),
  Method = c("Fisher Information", "Fisher Information"),
  Lower = c(ci_mu[1], ci_s[1]),
  Upper = c(ci_mu[2], ci_s[2])
)

# Add the non-parametric CIs as separate rows
ci_table <- rbind(
  ci_table,
  data.frame(
    Parameter = c("mu", "s"),
    Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
    Lower = c(ci_mu_np[1], ci_s_np[1]),
    Upper = c(ci_mu_np[2], ci_s_np[2])
  )
)

# Print the table
print(ci_table)
```

Notice that the Fisher information CIs are much smaller than the empirical CDF CIs. This is because the Fisher information CIs are based on the MLEs and assume the data have an underlying logistic distribution, thus the 95% CI is narrower and more reliable. On the other hand, the empirical CDF CIs are calculated using the range of the bootstrap samples which is sensitive to extreme values thus conflating the 95% CIs.
