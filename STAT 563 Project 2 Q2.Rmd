---
title: "STAT_563_project_2_Q2"
author: "Cecilia Albert-Black"
date: "2025-11-03"
output: html_document
---

This project integrated large-sample theory, likelihood-based inference, and re-sampling techniques in modern computational statistics. In this Markdown file, I will:

-   Examine the logistic distribution and its importance in statistical modeling and machine learning

-   compute confidence intervals based on Fisher information

-   Explore bootstrapping and the distribution of correlation matrices

-   Implement slice sampling for correlation uncertainty

### Finding the Maximum Likelihood Estimates (MLEs)

Given observations $x_1, \ldots, x_n$, the likelihood function is given by

$$
L(\mu, s) = s^{-n} \prod_{i=1}^{n} 
\frac{\exp\left(-\frac{x_i - \mu}{s}\right)}
{\left(1 + \exp\left(-\frac{x_i - \mu}{s}\right)\right)^2}.
$$

and taking logarithms we have the log-likelihood

$$
\ell(\mu, s) = -n \log s 
- \frac{1}{s} \sum_{i=1}^{n} (x_i - \mu)
- 2 \sum_{i=1}^{n} \log\left(1 + \exp\left(-\frac{x_i - \mu}{s}\right)\right).
$$

Let $z_i = (x_i - \mu)/s$, we obtain the score equations:

$$
\frac{\partial \ell}{\partial \mu} 
= \frac{1}{s} \sum_{i=1}^{n} \tanh\left(\frac{z_i}{2}\right),
$$

$$
\frac{\partial \ell}{\partial s} 
= -\frac{n}{s} 
+ \frac{1}{s^2} \sum_{i=1}^{n} (x_i - \mu)
- \frac{2}{s^2} \sum_{i=1}^{n} (x_i - \mu) 
\frac{e^{-z_i}}{1 + e^{-z_i}}.
$$

So, the score vector is:

$$
S = 
\begin{pmatrix}
\frac{\partial \ell}{\partial \mu} \\
\frac{\partial \ell}{\partial s}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{s} \sum_{i=1}^{n} \tanh\left(\frac{z_i}{2}\right) \\
-\frac{n}{s} + \frac{1}{s^2} \sum_{i=1}^{n} (x_i - \mu)
- \frac{2}{s^2} \sum_{i=1}^{n} (x_i - \mu)
\frac{e^{-z_i}}{1 + e^{-z_i}}
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0
\end{pmatrix}.
$$

**Plain language summary:**

The likelihood function is a joint probability or density of the observed data viewed as s function of the mean parameter (mu), not of the data. It's purtpose if the quantify how likely the observed data are for different mu.

The log-likelihood function is used to turn products into sums, effectively simplifying the calculation of the likelihood function. It also makes taking derivatives of it easier, which is important for optimization. Lastly, since logarithms are monotone, maximizing the log-likelihood function is equivalent to maximizing the likelihood function.

The score is the derivative of the log-likelihood with respect to the parameters (mu); it measured how sensitive the likelihood is to changes in mu. Solving for the score gives the MLE; in other words, the parameter values that maximize the likelihood.

The score vector is central for inference, such as constructing Wald tests, likelihood ratio tests, and score tests. Its covariance gives the Fisher Information, which measures how much information the data carries about mu.

Practical applications according to ChatGPT:

-   Statistics:

    -   MLE for estimating parameters in almost any parametric model (Normal, Logistic, Poisson, etc.).

    -   Hypothesis testing (score test, likelihood ratio test).

-   Machine Learning

    -   Logistic regression: Maximize log-likelihood for the coefficients.

    -   Neural networks: Likelihood of outputs leads to cross-entropy loss.

    -   Probabilistic models (Bayesian inference often uses likelihoods in posterior computation).

#### a. Show that the MLEs $$ \hat{\mu}, \hat{s} $$ are:

$$
\sum_{i=1}^{n} \tanh\left( \frac{X_i - \hat{\mu}}{2 \hat{s}} \right) = 0, \hat{s} = \frac{1}{n} \sum_{i=1}^{n} \lvert X_i - \hat{\mu} \rvert \, w_i
$$

And obtain $$ w_i $$

```{r}
# Create a random dataset
set.seed(123)  # for reproducibility

# Generate a random dataset of 50 points from a normal distribution
n <- 50
X <- rnorm(n, mean = 5, sd = 2)  

# Optional: add a few outliers
X <- c(X, 20, 25)

# Quick look at the dataset
hist(X)
```

```{r}
# 
n <- length(X)

# Define a function to compute mu_hat and s_hat iteratively
estimate_mu_s <- function(X, tol = 1e-6, max_iter = 1000) {
  
  # Initialize
  mu_hat <- median(X)   # robust starting value
  s_hat <- mean(abs(X - mu_hat))  # initial scale estimate
  
  for (iter in 1:max_iter) {
    
    # Compute weights
    z <- (X - mu_hat) / (2 * s_hat)
    w <- ifelse(z == 0, 1, tanh(z) / z)
    
    # Update mu_hat
    mu_new <- sum(X * w) / sum(w)
    
    # Update s_hat
    s_new <- mean(abs(X - mu_new) * w)
    
    # Check convergence
    if (abs(mu_new - mu_hat) < tol && abs(s_new - s_hat) < tol) {
      break
    }
    
    mu_hat <- mu_new
    s_hat <- s_new
  }
  
  list(mu_hat = mu_hat, s_hat = s_hat, weights = w, iterations = iter)
}

# Run the function
result <- estimate_mu_s(X)

# View results
cat("Results of Robust Estimation:\n")
cat("-----------------------------\n")
cat("Estimated mu_hat:", result$mu_hat, "\n")
cat("Estimated s_hat:", result$s_hat, "\n")
cat("Number of iterations:", result$iterations, "\n")
cat("\nWeights for each observation:\n")
print(result$weights)

```

The weights ( $$ w_i $$ ) represent the score/weight of each observation ( $$ X_i $$ ), telling us how trustworthy that data point is when calculating the $$ \hat{\mu}, \hat{s} $$. The closer the observation is to $$ \hat{\mu} $$ , the closer the weight is to 1, which means it is very trustworthy. The farther the observation is to $$ \hat{\mu} $$ , the farther the weight is from 1, which means it is not trustworthy. This effectively makes $$ \hat{\mu}, \hat{s} $$ robust estimators to outliers, since outlying values are given less importance than ones closer to the $$ \hat{\mu} $$ .

#### b. Fisher Information

Taking the second partial derivative, the observed information at ($$ \hat{\mu}, \hat{s} $$) is:

$$
\mathcal{F}_{\text{obs}}(\hat{\mu}, \hat{s}) = - \nabla^2 \ell(\hat{\mu}, \hat{s}) = - \mathcal{H}(\hat{\mu}, \hat{s}).
$$

The expected Fisher information per observation $$
\mathcal{F}(\mu, s) = 
\begin{pmatrix}
\dfrac{1}{3 s^2} & 0 \\[2mm]
0 & \dfrac{1}{s^2} \left( \dfrac{\pi^2}{3} - 1 \right)
\end{pmatrix}, 
\quad 
\mathcal{F}_n(\mu, s) = n \, \mathcal{F}(\mu, s).
$$

Generate random numbers from logistic distribution of sample size n = 200 observations and write a Fisher scoring iterative algorithm to find the MLEs and estimate the parameters (μ, s) of the logistic distribution.

```{r}
# Load necessary library
library(stats)

# Step 1: Generate random numbers from a logistic distribution
set.seed(123)  # For reproducibility
n <- 200
true_mu <- 0
true_s <- 1

# Logistic random variables
x <- rlogis(n, location = true_mu, scale = true_s)

# Step 2: Define the log-likelihood and its derivatives
loglik <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  n  <- length(data)
  
  # Logistic log-likelihood
  ll <- -n*log(s) - 2*sum(log(1 + exp(-(data - mu)/s)))
  return(ll)
}

# Score function (gradient of log-likelihood)
score <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  
  z <- (data - mu)/s
  w <- exp(-z) / (1 + exp(-z))
  
  # Partial derivatives
  dL_dmu <- sum((1 - 2*w)/s)
  dL_ds  <- -length(data)/s + sum(z * (1 - 2*w)/s)
  
  return(c(dL_dmu, dL_ds))
}

# Fisher information matrix (expected value of negative second derivative)
fisher_info <- function(params, data) {
  mu <- params[1]
  s  <- params[2]
  
  # Expected Fisher information for logistic distribution
  n <- length(data)
  I11 <- n * (pi^2) / (3 * s^2)       # w.r.t mu^2
  I12 <- 0                            # cross term
  I22 <- n * (pi^2) / (3 * s^2)       # w.r.t s^2
  
  I <- matrix(c(I11, I12, I12, I22), nrow = 2)
  return(I)
}

# Step 3: Fisher Scoring Algorithm
fisher_scoring <- function(data, tol = 1e-6, max_iter = 100) {
  # Initial guesses
  mu_hat <- mean(data)
  s_hat  <- sd(data) * sqrt(3)/pi  # Approximation for logistic scale
  
  params <- c(mu_hat, s_hat)
  
  for (i in 1:max_iter) {
    score_val <- score(params, data)
    info_val  <- fisher_info(params, data)
    
    # Update parameters
    params_new <- params + solve(info_val) %*% score_val
    
    # Check convergence
    if (max(abs(params_new - params)) < tol) {
      cat("Converged in", i, "iterations\n")
      break
    }
    
    params <- params_new
  }
  
  names(params) <- c("mu_hat", "s_hat")
  return(params)
}

# Step 4: Run Fisher scoring to estimate MLEs
mle_estimates <- fisher_scoring(x)
print(mle_estimates)

```

Output explanation according to ChatGPT:

1.  Convergence message
    -   This means the iterative Fisher scoring algorithm stopped after 77 iterations because the parameter estimates changed less than the specified tolerance (`tol = 1e-6`) between successive updates.
    -   Essentially, the algorithm believes it has reached the MLEs.
2.  Parameter estimates
    -   `mu_hat = 0.032` is the MLE for the location parameter μ of your logistic distribution. Since the true μ used to generate the data was 0, this estimate is very close—slight deviation is normal due to random sampling.
    -   `s_hat = 0.905` is the MLE for the scale parameter s of your logistic distribution. The true scale used was 1, so again this estimate is slightly lower, which is reasonable for a sample of size 200.

#### c. Large-sample Wald intervals

By inverting the Fisher information ( $$ \mathcal{F}_{\text{obs}} $$ ) or using the empirical CDF ( $$ \mathcal{F}_{\text{n}} $$ ) and using the generated data in part b, compute the confidence intervals:

1.  Using the Fisher information: more common in MLE theory

```{r}
# Extract MLEs from part b
mu_hat <- mle_estimates["mu_hat"]
s_hat  <- mle_estimates["s_hat"]
n      <- length(x)

# Construct the Fisher information matrix for the logistic distribution, evaluated at the MLEs
I11 <- n * (pi^2)/(3 * s_hat^2)
I22 <- n * (pi^2)/(3 * s_hat^2)
I12 <- 0
F_obs <- matrix(c(I11, I12, I12, I22), nrow = 2)

# Variance-covariance matrix
vcov <- solve(F_obs)

# Standard errors
se_mu <- sqrt(vcov[1,1])
se_s  <- sqrt(vcov[2,2])

# 95% confidence intervals
z <- qnorm(0.975)
ci_mu <- mu_hat + c(-1,1)*z*se_mu
ci_s  <- s_hat  + c(-1,1)*z*se_s

ci_mu
ci_s
```

2.  Using the empirical CDF: a non-parametric approach, less common but useful for CIs

```{r}
# Non-parametric CI for mu (location) using the empirical CDF
ci_mu_np <- quantile(x, probs = c(0.025, 0.975))
ci_mu_np

# Non-parametric CI for s (scale) using bootstrap
set.seed(123)   # for reproducibility
B <- 1000       # number of bootstrap samples

s_boot <- replicate(B, {
  xb <- sample(x, replace = TRUE)       # bootstrap sample
  diff(range(xb)) / (pi/sqrt(3))        # approximate logistic scale
})

# 95% CI for s from bootstrap percentiles
ci_s_np <- quantile(s_boot, probs = c(0.025, 0.975))
ci_s_np
```

Summary table

```{r}
# Create a data frame to display the results
ci_table <- data.frame(
  Parameter = c("mu", "s"),
  Method = c("Fisher Information", "Fisher Information"),
  Lower = c(ci_mu[1], ci_s[1]),
  Upper = c(ci_mu[2], ci_s[2])
)

# Add the non-parametric CIs as separate rows
ci_table <- rbind(
  ci_table,
  data.frame(
    Parameter = c("mu", "s"),
    Method = c("Empirical / Bootstrap", "Empirical / Bootstrap"),
    Lower = c(ci_mu_np[1], ci_s_np[1]),
    Upper = c(ci_mu_np[2], ci_s_np[2])
  )
)

# Print the table
print(ci_table)
```

Notice that the Fisher information CIs are much smaller than the empirical CDF CIs. This is because the Fisher information CIs are based on the MLEs and assume the data have an underlying logistic distribution, thus the 95% CI is narrower and more reliable. On the other hand, the empirical CDF CIs are calculated using the range of the bootstrap samples which is sensitive to extreme values thus conflating the 95% CIs.
